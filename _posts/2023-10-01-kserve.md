---
layout: single
title: Deploying Machine Learning Models with KServe in K8s 
date: 2023-09-05
type: post
published: false
status: publish
categories: 
- security
tags: ["security", "Kubernetes", "Cloud", "OpenShift", "Azure"]
author: rcarrata
comments: true
---

What is the role of Application Gateway in enabling the secure exposure of customer applications within Private Azure Red Hat OpenShift (ARO) clusters?
How does Application Gateway integrate with Private ARO clusters and align with the connectivity strategy of Open Hybrid Cloud?
What are the benefits of using Application Gateway for load balancing customer applications within ARO clusters, especially during high demand scenarios?

## Overview



## Kserve

KServe stands out as a remarkably scalable toolkit for deploying machine learning models on Kubernetes. It serves as an orchestration tool that operates atop Kubernetes and harnesses the capabilities of two other open-source projects, namely Knative-Serving and Istio, which we will delve into further in the following sections.

TODO: Insert image here!

KServe makes it much easier to deploy machine learning models in a Kubernetes cluster. It does this by simplifying the deployment process and combining it into a single resource definition. This makes it simpler for anyone working on a machine learning project to understand and reduces the difficulty of getting started. Models deployed using KServe can also be easier to maintain compared to traditional Kubernetes deployments that require setting up Flask or FastAPI services.

With KServe, there's no need to wrap your model inside a FastAPI or Flask application before making it available over the internet through HTTPS. KServe has built-in functionality that essentially replicates this process without the complexity of managing API endpoints, configuring pod replicas, or setting up internal routing networks in Kubernetes. All you need to do is point KServe to your model, and it will take care of the rest.

In addition to simplifying deployment, KServe offers various features like canary deployments, automatic scaling for inference, and request batching. We won't go into detail about these features in this guide as they are beyond its scope. However, this guide should provide a solid foundation for understanding KServe.

Now, let's discuss the two main technologies that work alongside KServe: Istio and Knative.

## Knative Serverless

Knative, in contrast, is an open-source solution designed for building enterprise-level applications that are serverless and event-driven. It leverages Istio as its foundation to provide serverless code execution capabilities akin to what AWS Lambdas and Azure Functions offer. Knative is a platform-agnostic solution designed for running serverless deployments within Kubernetes.

One of Knative's standout features is its "scale-to-zero" capability. This particular feature plays a vital role in enabling KServe to efficiently scale machine learning model deployments up or down. It optimizes resource usage and cost savings by dynamically adjusting the available resources based on demand.

##

*NOTE: Opinions expressed in this blog are my own and do not necessarily reflect that of the company I work for.*

Happy OpenShifting!

<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="rcarrata" data-color="#FFDD00" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee :)" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff" ></script>