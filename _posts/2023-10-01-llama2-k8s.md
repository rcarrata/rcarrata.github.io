---
layout: single
title: Deploying Private chatGPT (Llama2 GGMF) in Kubernetes without GPU!
date: 2023-09-05
type: post
published: false
status: publish
categories: 
- security
tags: ["security", "Kubernetes", "Cloud", "OpenShift", "Azure"]
author: rcarrata
comments: true
---

What is the role of Application Gateway in enabling the secure exposure of customer applications within Private Azure Red Hat OpenShift (ARO) clusters?
How does Application Gateway integrate with Private ARO clusters and align with the connectivity strategy of Open Hybrid Cloud?
What are the benefits of using Application Gateway for load balancing customer applications within ARO clusters, especially during high demand scenarios?

## Overview

##


## Prerequisites

* Deploy [Kind cluster with GPU](https://www.substratus.ai/blog/kind-with-gpus)

* Deploy Nginx Ingress Controller:

```md
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/kind/deploy.yaml
```

* Deploy Nvidia GPU Operator

```md
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia || true
helm repo update
helm install --wait --generate-name \
     -n gpu-operator --create-namespace \
     nvidia/gpu-operator --set driver.enabled=false
```

* Deploy Pod to Check nvidia-smi
```md
kubectl apply -f - << EOF
apiVersion: v1
kind: Pod
metadata:
  name: cuda-vectoradd
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-vectoradd
    image: "nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04"
    resources:
      limits:
        nvidia.com/gpu: 1
EOF
docker exec -ti k8s-control-plane ln -s /sbin/ldconfig /sbin/ldconfig.real
kubectl delete --all pod -n gpu-operator
```

## Deploy Llama2 in Kubernetes

* Deploy Llama2 in Kubernetes

```md
kubectl apply -k manifests/overlays/
```


```
$ kubectl logs -n k8s-llama2 k8s-llama2-6b8cfbb697-kbg9t
Downloading (…)chat.ggmlv3.q5_1.bin: 100%|██████████| 9.76G/9.76G [03:22<00:00, 48.1MB/s]
llama.cpp: loading model from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 4096
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_head_kv  = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: n_gqa      = 1
llama_model_load_internal: rnorm_eps  = 5.0e-06
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: freq_base  = 10000.0
llama_model_load_internal: freq_scale = 1
llama_model_load_internal: ftype      = 9 (mostly Q5_1)
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =    0.11 MB
llama_model_load_internal: mem required  = 9311.07 MB (+ 3200.00 MB per state)
lama_new_context_with_model: kv self size  = 3200.00 MB
llama_new_context_with_model: compute buffer total size =  351.35 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |
```

```md
kubectl logs -f k8s-llama2-55478bcf8-mns7s -n k8s-llama2 | grep Kubernetes -A10
 Kubernetes is an open-source container orchestration system for automating deployment, scaling, and management of containerized applications. It was originally designed by Google, and is now maintained by the Cloud Native Computing Foundation (CNCF). Kubernetes allows you to deploy, manage, and scale containerized applications in a flexible, efficient, and highly available manner. It provides a platform-agnostic way to manage your applications, and supports a wide range of operating systems and infrastructure providers.''

llama_print_timings:        load time = 25720.72 ms
llama_print_timings:      sample time =   101.65 ms /   106 runs   (    0.96 ms per token,  1042.81 tokens per second)
llama_print_timings: prompt eval time = 25720.67 ms /    41 tokens (  627.33 ms per token,     1.59 tokens per second)
llama_print_timings:        eval time = 83628.51 ms /   105 runs   (  796.46 ms per token,     1.26 tokens per second)
llama_print_timings:       total time = 109751.86 ms
```


*NOTE: Opinions expressed in this blog are my own and do not necessarily reflect that of the company I work for.*

Happy OpenShifting!

<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="rcarrata" data-color="#FFDD00" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee :)" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff" ></script>